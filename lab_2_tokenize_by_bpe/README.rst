Лабораторная работа №2
======================


.. toctree::
    :maxdepth: 1
    :titlesonly:
    :caption: Full API

    lab_2_tokenize_by_bpe.api.rst


Дано
----

1. `Текст на русском языке <assets/text.txt>`__ (загружен и сохранен в
   переменную ``text`` в ``start.py``).
2. Секретный зашифрованный текст
3. `Обученный на большом корпусе текстов словарь
   токенов <assets/vocab.json>`__
4. `Нейросетевая языковая модель <assets/nmt_demo>`__ (`пример
   обращения <assets/nmt_demo/main.py>`__)

По ходу выполнения лабораторной работы необходимо научиться
предобрабатывать текст при помощи алгоритма `Byte-Pair
Encoding <https://en.wikipedia.org/wiki/Byte_pair_encoding>`__ таким
образом, чтобы суметь получить предсказания от нейросетевой
лингвистической модели и оценить ее работу.

**Важно:** В рамках данной лабораторной работы **нельзя использовать
сторонние модули, а также стандартные модули collections и itertools.**

Терминология
------------

В данной лабораторной работе мы будем оперировать следующими понятиями:

-  **слово** - последовательность символов между пробельными символами;
-  **предобработанное слово** - слово, разбитое на токены;
-  **частота слова** - количество вхождений слова в текст;
-  **токен** - часть, выделяемая в слове;
-  **идентификатор токена** - целочисленное значение, однозначно
   указывающее на определенный токен;
-  **vocabulary (словарь токенов)** - словарь, устанавливающий
   однозначное соответствие между токенами и идентификаторами токенов.

Описание алгоритма BPE (Byte-Pair Encoding)
-----------------------------------------------

Целью автоматической обработки естественного языка (Natural Language
Processing, NLP) является формализация языка таким образом, чтобы
сохранить его смысловое наполнение. Ключевую роль в такой обработке
играет токенизация и кодирование текста. Благодаря этому становится
возможным использование формальных языковых моделей, которые не способны
работать с текстовыми данными в первоначальном виде, но широко
используются для выявления скрытых закономерностей в числовых данных.

В качестве самого простого способа токенизировать текст можно
рассмотреть разбиение текста на слова. Однако у такого подхода есть
существенные минусы. Во-первых, размер словаря (vocabulary) -
множества всех уникальных токенов - в таком случае получается достаточно
большим. Во-вторых, даже при таком очень большом словаре нередко
приходится сталкиваться с проблемой
`Out-of-Vocabulary <https://blog.marketmuse.com/glossary/out-of-vocabulary-oov-definition/>`__
слов. Это не позволяет сделать обработку универсальной и масштабируемой
на большое количество дискурсов.

По этой причине сегодня существует множество техник кодирования текста
по частям слов. Одной из них является Byte-Pair Encoding, с которым вам
и предстоит поработать в рамках данной лабораторной работы.

Данный алгоритм был предложен в 1994 Филиппом Гейджем в статье `“Новый
метод сжатия
данных” <http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM>`__.
Идея алгоритма сводится к замене самых частотных пар символов другим
символом, при этом объем используемой памяти снижается в два раза.

В контексте токенизации текста BPE представляет собой итеративное
слияние наиболее частых пар последовательных токенов в тексте (или
корпусе текстов) до тех пор, пока не будет достигнут определенный размер
словаря.

Для лучшего понимания рассмотрим применение этого алгоритма на примере.

Допустим, нам дан следующий текст:

.. code:: py

   "It's far, farther, farthest and old, older, oldest"

Разобьем текст на слова по пробельным символам и посимвольно
токенизируем каждое слово:

.. code:: py

   [
       ('I', 't', "'", 's', '</s>'),
       ('f', 'a', 'r', ',', '</s>'),
       ('f', 'a', 'r', 't', 'h', 'e', 'r', ',', '</s>'),
       ('f', 'a', 'r', 't', 'h', 'e', 's', 't', '</s>'),
       ('a', 'n', 'd', '</s>'),
       ('o', 'l', 'd', ',', '</s>'),
       ('o', 'l', 'd', 'e', 'r', ',', '</s>'),
       ('o', 'l', 'd', 'e', 's', 't', '</s>')
   ]

Обратите внимание, что каждое слово заканчивается специальным токеном
``'</s>'``, обозначающим конец слова.

Составим таблицу частотности пар токенов. Нас интересуют те токены,
которые идут друг за другом:

.. code:: py

   {
       ('I', 't'): 1, ('t', "'"): 1, ("'", 's'): 1, ('s', '</s>'): 1,
       ('f', 'a'): 3, ('a', 'r'): 3, ('r', ','): 3, (',', '</s>'): 4,
       ('r', 't'): 2, ('t', 'h'): 2, ('h', 'e'): 2, ('e', 'r'): 2,
       ('e', 's'): 2, ('s', 't'): 2, ('t', '</s>'): 2,
       ('a', 'n'): 1, ('n', 'd'): 1, ('d', '</s>'): 1,
       ('o', 'l'): 3, ('l', 'd'): 3, ('d', ','): 1,
       ('d', 'e'): 2
   }

Обратите внимание, что мы не рассматриваем пары токенов, встречающихся
на границе слов. Поэтому у нас нет пар, начинающихся с ``'</s>'``.

Теперь необходимо выбрать самую часто встречающуюся пару. В нашем
примере ею является пара токенов ``(',', '</s>')``. Давайте объединим ее
в один токен:

.. code:: py

   [
       ('I', 't', "'", 's', '</s>'),
       ('f', 'a', 'r', ',</s>'),
       ('f', 'a', 'r', 't', 'h', 'e', 'r', ',</s>'),
       ('f', 'a', 'r', 't', 'h', 'e', 's', 't', '</s>'),
       ('a', 'n', 'd', '</s>'),
       ('o', 'l', 'd', ',</s>'),
       ('o', 'l', 'd', 'e', 'r', ',</s>'),
       ('o', 'l', 'd', 'e', 's', 't', '</s>')
   ]

Далее мы пересчитываем встречаемость пар токенов, соединяем самую частую
пару и повторяем процесс до тех пор, пока не достигнем нужного размера
словаря.

Итогом работы алгоритма должно стать следующее разбиение:

.. code:: py

   [
       ('I', 't', "'", 's', '</s>'),
       ('far', ',</s>'),
       ('far', 'th', 'er', ',</s>'),
       ('far', 'th', 'est', '</s>'),
       ('a', 'n', 'd', '</s>'),
       ('old', ',</s>'),
       ('old', 'er', ',</s>'),
       ('old', 'est', '</s>')
   ]

Предполагается, что таким образом удается выделить из текста значимые
последовательности: это могут быть корни слов либо другие широко
употребляемые морфемы. Так, в нашем примере нам удалось выделить два
корня (``far`` и ``old``), а также суффиксы сравнительной и превосходной
степени ``er`` и ``est``.

Иногда в алгоритме вместо специального токена конца слова используется
токен, обозначающий, напротив, начало слова. В настоящей лабораторной
нам доведется столкнуться и с тем, и с другим.

Давайте пошагово рассмотрим реализацию такого алгоритма:

1. Создание частотного словаря корпуса, отражающего
   количество вхождений каждого из слов.
2. Токенизация слов (на первой итерации - разделение слов на
   символы).
3. Составление частотного словаря пар токенов, которые следуют
   друг за другом в рамках одного слова.
4. Формирование нового токена из самой часто встречающейся пары.
5. Повторение шагов 2-4 до тех пор, пока
   не будет достигнуто желаемое количество токенов.

Что надо сделать
----------------

Шаг 0. Подготовка (проделать вместе с преподавателем на практике)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Изменить файлы ``main.py`` и ``start.py``
2. Закоммитить изменения и создать новый Pull Request

**Важно**: Код, выполняющий все требуемые действия должен быть написан в
функции ``main`` в модуле ``start.py``. Для этого реализуйте функции в
модуле ``main.py`` и импортируйте их в ``start.py``.

.. code:: py

   if __name__ == '__main__':
       main()

Обратите внимание, что в файле
`target_score.txt <target_score.txt>`__ необходимо выставить
желаемую оценку: 4, 6, 8 или 10. Чем выше желаемая оценка, тем большее
количество тестов запускается при проверке вашего Pull Request.

Шаг 1. Токенизация одного слова
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Начнем с предобработки слов.

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.prepare_word`.

Функция не должна удалять из строки специальные символы или приводить
текст к нижнему регистру. В случае, если какой-либо из специальных токен,
обозначающих начало и конец слова, представлен значением ``None``,
то добавлять его в кортеж не требуется.

Например, строка ``"It's"`` должна быть обработана следующим образом:
``('I', 't', "'", 's', '</s>')``, где ``'</s>'`` - токен, обозначающий
конец слова, в качестве токена, обозначающего начало слова, было
передано ``None``.

Шаг 2. Формирование частотного словаря
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: Выполнение Шага 2 соответствует 4 баллам

Далее необходимо собрать частотный словарь, ключами которого выступают
предобработанные слова, а значениями - количество вхождений слов в
текст. В рамках данной лабораторной работы будем придерживаться мнения,
что граница слова - это любой пробельный символ.

Например, из текста
``"It's far, farther, farthest and old, older, oldest"`` должен
получиться словарь следующего вида:

.. code:: py

   {
       ('I', 't', "'", 's', '</s>'): 1,
       ('f', 'a', 'r', ',', '</s>'): 1,
       ('f', 'a', 'r', 't', 'h', 'e', 'r', ',', '</s>'): 1,
       ('f', 'a', 'r', 't', 'h', 'e', 's', 't', '</s>'): 1,
       ('a', 'n', 'd', '</s>'): 1,
       ('o', 'l', 'd', ',', '</s>'): 1,
       ('o', 'l', 'd', 'e', 'r', ',', '</s>'): 1,
       ('o', 'l', 'd', 'e', 's', 't', '</s>'): 1
   }

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.collect_frequencies`.
При этом токен начала слова может быть представлен ``None``, а токен конца - нет.
Функция обязательно должна вызывать функцию
:py:func:`lab_2_tokenize_by_bpe.main.prepare_word`.

Продемонстрируйте составление частотного словаря в функции ``main()``
модуля ``start.py``, используя текст на русском языке (переменная
``text``). В качестве токена окончания слова используйте строку
``</s>``. Токен начала слова не используйте, то есть передайте в его
качестве ``None``.

Шаг 3. Подсчет количества вхождений каждой из пар токенов
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Для формирования новых токенов необходимо выделить самые часто
встречающиеся сочетания уже существующих токенов. Для этого нужно
сформировать частотный словарь, в котором в качестве ключей используются
пары существующих токенов (то есть пока что только пары символов), а
значениями - количество случаев, когда эти символы следуют друг за
другом в пределах одного слова.

Так, для примера из предыдущего шага частотный словарь сочетания токенов
имеет следующий вид:

.. code:: py

   {
       ('I', 't'): 1, ('t', "'"): 1, ("'", 's'): 1, ('s', '</s>'): 1,
       ('f', 'a'): 3, ('a', 'r'): 3, ('r', ','): 3, (',', '</s>'): 4,
       ('r', 't'): 2, ('t', 'h'): 2, ('h', 'e'): 2, ('e', 'r'): 2,
       ('e', 's'): 2, ('s', 't'): 2, ('t', '</s>'): 2,
       ('a', 'n'): 1, ('n', 'd'): 1, ('d', '</s>'): 1,
       ('o', 'l'): 3, ('l', 'd'): 3, ('d', ','): 1,
       ('d', 'e'): 2
   }

Обратите внимание, что сочетания токенов на стыке слов не образуют пару.
Так, в нашем словаре пар нет сочетаний, начинающихся с токена конца
слова. Также обратите внимание, что для пары порядок токенов критичен.

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.count_tokens_pairs`.

Шаг 4. Формирование нового токена
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Словарь частотности сочетаний токенов позволяет нам выбрать, из чего
можно сформировать новый токен.

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.merge_tokens`,
которая обновляет частотный словарь слов, заменяя в ключах пары токенов,
из которых сформирован новый токен, на этот самый новый объединенный токен.

Так, если в частотном словаре слов из предыдущего шага объединить пару
токенов ``(',', '</s>')`` в новый токен, то словарь должен измениться
следующим образом:

.. code:: py

   {
       ('I', 't', "'", 's', '</s>'): 1,
       ('f', 'a', 'r', ',</s>'): 1,
       ('f', 'a', 'r', 't', 'h', 'e', 'r', ',</s>'): 1,
       ('f', 'a', 'r', 't', 'h', 'e', 's', 't', '</s>'): 1,
       ('a', 'n', 'd', '</s>'): 1,
       ('o', 'l', 'd', ',</s>'): 1,
       ('o', 'l', 'd', 'e', 'r', ',</s>'): 1,
       ('o', 'l', 'd', 'e', 's', 't', '</s>'): 1
   }

Шаг 5. Обучение токенизатора
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: Выполнение Шага 5 соответствует 6 баллам

Теперь у нас есть все компоненты для того, чтобы обучить наш токенизатор
и сформировать необходимое количество новых токенов.

Для этого реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.train`.

Для формирования нового токена необходимо выбирать самую часто
встречающуюся пару токенов. В случае, если несколько пар встречаются
одинаково часто, необходимо выбрать ту, которая даст более длинный
токен. Если и это не даст однозначного ответа, то необходимо выбрать ту
пару, которая дает лексикографически меньший токен.

Например, имея пары ``{('a', 'b'): 3, ('b', 'cd'): 3, ('b', 'ca'): 3}``
необходимо сформировать токен ``'bca'``, так этот токен длиннее токена
``'ab'`` и лексикографически меньше токена ``'bcd'``.

Обратите внимание, что если доступных для слияния пар токенов не
осталось, то обучение должно прекратиться даже если необходимое
количество токенов не было достигнуто.

Продемонстрируйте обучение токенизатора на материале текста из
переменной ``text`` в модуле ``start.py``. Используйте 100 слияний в
качестве критерия остановки обучения.

.. note:: Подумайте, можно ли установить связь между оптимальным
          количеством слияний и количеством вхождений в частотный словарь слов?

Шаг 6. Присвоение идентификаторов токенам
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Известно, что формальные модели, включая лингвистические модели, не
способны обрабатывать буквенные данные, поэтому необходимо для каждой
текстовой последовательности сформировать числовой вектор.

Для этого необходимо присвоить каждому из получившихся токенов
определенный числовой идентификатор. Отсортируйте токены по длине в
порядке убывания. Токены, имеющие одинаковую длину, должны быть
отсортированы лексикографически в порядке возрастания. Присвойте данной
последовательности идентификаторы от 0 до n-1, где n - количество
токенов.

Например, из набора токенов
``['far', 'old', 'th', 'er', 'est', '</s>', 'a', 'n', 'd']`` должен
получиться следующий словарь:

.. code:: py

   {'<unk>': 0, 'est': 1, 'far': 2, 'old': 3, '</s>': 4, 'er': 5, 'th': 6, 'a': 7, 'd': 8, 'n': 9}

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.get_vocabulary`.
В возвращаемом словаре должны присутствовать специальные токены: токен
конца слова (при наличии), токен начала слова (при наличии) и
неизвестный токен. Присвоение идентификатора данным токенам производится
так же после сортировки вместе с остальными токенами.

Шаг 7. Декодирование текста
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: Выполнение Шага 7 соответствует 8 баллам

Теперь, когда у нас есть выделенные токены и присвоенные им числовые
идентификаторы, мы можем декодировать любую последовательность.

Для этого реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.decode`.
Обратите внимание, что в возвращаемой строке не
должно быть токенов, обозначающих конец слова! Вместо них должно быть
то, что мы в рамках текущей работы считаем границей слова, то есть
пробельный символ.

Так, при использовании словаря из предыдущего шага, последовательность
``[2, 6, 1, 4]`` должна быть раскодирована как ``'farthest '``.

Создайте словарь вида ``<токен: числовой идентификатор>``, используя
``'<unk>'`` в качестве токена, обозначающего неизвестную
последовательность. Затем продемонстрируйте работу декодера, прочитав и
декодировав секретное содержимое любого файла из папки
`./assets/secrets <assets/secrets>`__ в модуле ``start.py``.

Обратите внимание, что данные файлы содержат инструкцию о получении
бонуса к оценке. **Количество бонусов ограничено**: только первые
студенты, справившиеся с заданием, смогут его получить. Один студент
может отгадать не более одной загадки, таким образом, бонус смогут
получить только 5 студентов. Решение о применении бонуса принимается
ментором и не подлежит оспариванию. Применение бонуса возможно только в
случае, если на момент выполнения задания в форке студента опубликован
актуальный код, позволяющий воспроизвести результат.

Шаг 8. Кодирование одного слова
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Кодирование текста, в отличие от декодирования, является более сложным
процессом и подразумевает предварительную токенизацию текста.
На текущем шаге вам нужно реализовать
функцию :py:func:`lab_2_tokenize_by_bpe.main.tokenize_word`.

Обратите внимание, что при токенизации слова необходимо в первую очередь
выбирать более длинные токены.

Например, мы токенизируем слово ``('w', 'o', 'r', 'd', '</s>')``
и имеем следующий словарь токенов:

.. code:: py

   {
       'w': 0, '<unk>': 1, 'o': 2, 'r': 3, 'd': 4, 'wo': 5, 'ord': 6, '</s>': 7
   }

При корректной реализации слово должно быть токенизировано следующим
образом: ``[0, 6, 7]``. Варианты ``[5, 3, 4]`` и ``[0, 2, 3, 4]``
являются неправильными: токен ``ord`` длиннее токенов ``wo`` и тем более
``w``, ``o``, ``r`` и ``d``.

В случае, если есть несколько подходящих самых длинных токенов, следует
использовать тот токен, который меньше лексикографически. В случае, если
встречается последовательность, которую нельзя заменить ни на один из
известных токенов, она заменяется на специальный неизвестный токен. Это
позволит без ошибок обрабатывать тексты, включающие в себя, например,
вставки на других языках.

Шаг 9. Загрузка словаря
~~~~~~~~~~~~~~~~~~~~~~~

В дальнейшем нам предстоит работать с нейросетевой языковой моделью,
обученной на задачу перевода с русского языка на английский. Для этого
необходимо использовать специальный словарь токенов, обученный на
большом объеме текстов, расположенный в файле
`./assets/vocab.json <assets/vocab.json>`__.

Таким образом, необходимо научиться считывать такие словари из файлов с
расширением ``.json``.

Мы уже встречались с файлами такого формата в предыдущих лабораторных
работах. Больше узнать об этом формате можно
`здесь <https://ru.wikipedia.org/wiki/JSON>`__.

Для работы с такими файлами используется библиотека
`json <https://pythonworld.ru/moduli/modul-json.html>`__.

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.load_vocabulary`.

Шаг 10. Кодирование текста
~~~~~~~~~~~~~~~~~~~~~~~~~~

Для того чтобы формальная модель поняла наш запрос, необходимо его
закодировать.

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.encode`.

Функция обязательно должна вызывать функции
:py:func:`lab_2_tokenize_by_bpe.main.prepare_word` и
:py:func:`lab_2_tokenize_by_bpe.main.tokenize_word`.

Шаг 11. Выделение N-грамм
~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь мы готовы перейти к тому, чтобы научиться оценивать качество
перевода. Для сравнения полученных предсказаний с истинным ответом,
необходимо научиться рассчитывать метрику `BLEU (bilingual evaluation
understudy) <https://en.wikipedia.org/wiki/BLEU>`__.

Эта метрика часто используется для оценки предсказаний в задаче
машинного перевода текста. Она представляет собой геометрическое среднее
метрик
`Precision <https://en.wikipedia.org/wiki/Precision_and_recall>`__,
посчитанных для n-грамм различного порядка.

В течение следующих нескольких шагов мы реализуем подсчет метрики BLEU.

Для начала необходимо реализовать функцию для выделения n-грамм из
последовательности :py:func:`lab_2_tokenize_by_bpe.main.collect_ngrams`.

N-граммами называют такую подпоследовательность,
которая включает в себя ровно n последовательных элементов. Например, из
буквенной последовательности ``мыть`` можно выделить четыре униграммы
(``м``, ``ы``, ``т``, ``ь``), три биграммы (``мы``, ``ыт``, ``ть``), две
триграммы (``мыт``, ``ыть``) и, наконец, одну 4-грамму (``мыть``).

Например, при вызове функции с последовательностью ``мыть`` и порядком
3, мы ожидаем получить результат ``[('м', 'ы', 'т'), ('ы', 'т', 'ь')]``.

Шаг 12. Вычисление метрики Precision
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

В общем случае значение метрики Precision обозначает, сколько
положительных предсказаний модели оказались действительно
положительными. В нашем случае можно сформулировать это более конкретно:
какая доля истинных значений попала в предсказания модели.

Иными словами, чем больше предсказанных n-грамм действительно
присутствуют в истинной последовательности, тем выше значение Precision.

Рассмотрим пример. Пусть истинными значениями является
``[('м',), ('ы',), ('т',), ('ь',)]``, а предсказанными -
``[('м',), ('ы',), ('т',), ('ь',), ('c',), ('я',)]``. Тогда совпадающими
значениями являются следующие 4 элемента:
``[('м',), ('ы',), ('т',), ('ь',)]``. Так как всего нам дано 6
предсказанных значений Precision, то доля совпадающих элементов, равна
:math:`\frac{4}{6}=0.(6)`.

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.calculate_precision`.

Шаг 13. Вычисление среднего геометрического
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

До расчета метрики BLEU нам остался всего один шаг, и это подсчет
среднего `геометрического
значения <https://ru.wikipedia.org/wiki/Среднее_геометрическое>`__.

Рассчитывать среднее значение мы будем для метрик Precision,
посчитанных для n-грамм различного порядка от 1 до :math:`order_{max}`,
где :math:`order_{max}` - максимальный порядок рассматриваемых n-грамм.

Рассчитать значение можно по следующей формуле:

.. math:: Mean_{geometric} = \frac{1}{order_{max}} \times \sum_{i=1}^{order_{max}}ln(Precision_{i})

Здесь :math:`Precision_{i}` обозначает значение метрики Precision для
n-грамм порядка :math:`i`, причем :math:`i` принимает значения от 1 до
:math:`order_max`.

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.geo_mean`.

Шаг 14. Вычисление метрики BLEU
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: Выполнение Шага 14 соответствует 10 баллам

Наконец, мы готовы рассчитать метрику BLEU, отражающую близость
предсказанного значения истинному.

Реализуйте функцию :py:func:`lab_2_tokenize_by_bpe.main.calculate_bleu`.

Функция должна выделить n-граммы из предоставленных последовательностей
всех порядков от 1 до обозначенного максимального, посчитать Precision
для выделенных n-грамм каждого порядка, вычислить среднее
геометрического полученных значений метрики и вернуть это значение,
предварительно умножив на 100.

Функция обязательно должна вызывать
:py:func:`lab_2_tokenize_by_bpe.main.collect_ngrams`,
:py:func:`lab_2_tokenize_by_bpe.main.calculate_precision` и
:py:func:`lab_2_tokenize_by_bpe.main.geo_mean`.

Продемонстрируйте оценку работы языковой модели
``Helsinki-NLP/opus-mt-ru-en`` в задаче машинного перевода в
``start.py``. Информацию об этой языковой модели можно получить
`здесь <https://huggingface.co/Helsinki-NLP/opus-mt-ru-en>`__. Пример
работы с данной языковой моделью вы можете увидеть
`здесь <assets/nmt_demo/main.py>`__.

Вам не нужно импортировать модель и генерировать предсказания, это
сделано за вас. Текст, использованный для получения предсказания,
расположен в файле
`./assets/for_translation_ru_raw.txt
<assets/for_translation_ru_raw.txt>`__.
Закодируйте его при помощи специального `обученного
словаря <assets/vocab.json>`__. При кодировании в качестве токена начала
слова следует использовать ``\u2581``, в качестве неизвестного токена
необходимо использовать ``<unk>``, токен конца слова использовать не
нужно (т.е. необходимо передать ``None``).

Чтобы убедиться, что файл удалось закодировать верным образом,
можно сравниться с закодированной версией текста из файла
`./assets/for_translation_ru_encoded.txt
<assets/for_translation_ru_encoded.txt>`__.
Закодированные предсказания модели (то есть предсказания в том виде, в
котором их вернула модель) можно найти в файле
`./assets/for_translation_en_encoded.txt
<assets/for_translation_en_encoded.txt>`__.
Необходимо декодировать их, используя реализованные вами функции, и
сравнить получившийся текст с эталонным переводом из файла
`./assets/for_translation_en_raw.txt
<assets/for_translation_en_raw.txt>`__.
При декодировании токен конца слова следует передать как ``None``.
Сравнение следует производить по метрике BLEU. После декодирования и
перед сравнением необходимо очистить текст от токена начала слова
(``\u2581``), заменив его на пробел.

Полезные ссылки
---------------

-  `Оригинальная статья о BPE
   алгоритме <http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM>`__
-  `Описание формата хранения данных JSON <https://ru.wikipedia.org/wiki/JSON>`__
   и `документация библиотеки <https://pythonworld.ru/moduli/modul-json.html>`__
   для работы с такими файлами
-  `Описание нейросетевой модели <https://huggingface.co/Helsinki-NLP/opus-mt-ru-en>`__
   чьи предсказания были использованы в настоящей
   работе
-  `Оригинальная статья о BLEU
   метрике <https://aclanthology.org/P02-1040.pdf>`__, используемой для
   оценки качества машинного перевода
